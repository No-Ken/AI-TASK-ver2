import axios from 'axios';
import sharp from 'sharp';
import { createHash } from 'crypto';

/**
 * OCRå‡¦ç†çµæœã®å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
 */
export interface OCRResult {
  text: string;
  confidence: number;
  language: string;
  blocks: TextBlock[];
  metadata: {
    imageWidth: number;
    imageHeight: number;
    processingTime: number;
    optimizationApplied: string[];
    estimatedCost?: number;
  };
}

export interface TextBlock {
  text: string;
  confidence: number;
  boundingBox: {
    x: number;
    y: number;
    width: number;
    height: number;
  };
  type: 'paragraph' | 'line' | 'word';
}

/**
 * SNSåˆ¥ã®ç”»åƒãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±
 */
export interface SNSLayout {
  platform: 'instagram' | 'tiktok' | 'twitter' | 'facebook' | 'youtube' | 'general';
  confidence: number;
  textRegions: Array<{
    name: string;
    region: { x: number; y: number; width: number; height: number };
    priority: number;
  }>;
  skipRegions: Array<{
    name: string;
    region: { x: number; y: number; width: number; height: number };
  }>;
}

/**
 * ç”»åƒå‰å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
 */
export interface ImageOptimizationOptions {
  maxWidth: number;
  maxHeight: number;
  quality: number;
  grayscale: boolean;
  contrast: number;
  brightness: number;
  sharpen: boolean;
  removeNoise: boolean;
}

/**
 * ã‚³ã‚¹ãƒˆåŠ¹ç‡çš„ãªç”»åƒOCRã‚µãƒ¼ãƒ“ã‚¹
 * å‰å‡¦ç†ã«ã‚ˆã‚‹ç”»åƒæœ€é©åŒ–ã¨SNSåˆ¥ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè§£æã§OCRã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›
 */
export class ImageOCRService {
  private readonly maxFileSize = 4 * 1024 * 1024; // 4MB
  private readonly cacheExpiry = 24 * 60 * 60 * 1000; // 24æ™‚é–“
  private readonly ocrCache = new Map<string, { result: OCRResult; timestamp: number }>();
  
  // OCR APIè¨­å®šï¼ˆè¤‡æ•°ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œï¼‰
  private readonly ocrProviders = {
    googleVision: {
      enabled: !!process.env.GOOGLE_VISION_API_KEY,
      costPerImage: 1.5, // USD per 1000 images
      accuracy: 0.95,
    },
    azureCV: {
      enabled: !!process.env.AZURE_CV_KEY,
      costPerImage: 1.0,
      accuracy: 0.93,
    },
    awsTextract: {
      enabled: !!process.env.AWS_TEXTRACT_ENABLED,
      costPerImage: 1.5,
      accuracy: 0.92,
    },
    tesseract: {
      enabled: true, // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
      costPerImage: 0.0,
      accuracy: 0.85,
    },
  };

  constructor() {
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®šæœŸå®Ÿè¡Œ
    setInterval(() => this.cleanupCache(), 60 * 60 * 1000); // 1æ™‚é–“ã”ã¨
  }

  /**
   * ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼‰
   */
  async extractTextFromImage(
    imageBuffer: Buffer,
    options: {
      preferredProvider?: string;
      maxCost?: number;
      forceReprocess?: boolean;
      contextHint?: string;
    } = {}
  ): Promise<OCRResult> {
    const startTime = Date.now();
    
    try {
      // ç”»åƒãƒãƒƒã‚·ãƒ¥ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
      const imageHash = this.generateImageHash(imageBuffer);
      
      if (!options.forceReprocess) {
        const cached = this.getCachedResult(imageHash);
        if (cached) {
          return cached;
        }
      }

      // ç”»åƒãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
      this.validateImage(imageBuffer);

      // SNSãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¤å®š
      const layout = await this.detectSNSLayout(imageBuffer);
      
      // ç”»åƒå‰å‡¦ç†ãƒ»æœ€é©åŒ–
      const optimizedImage = await this.optimizeImageForOCR(imageBuffer, layout);
      
      // æœ€é©ãªOCRãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
      const provider = this.selectOptimalProvider(options);
      
      // OCRå®Ÿè¡Œ
      const ocrResult = await this.performOCR(optimizedImage, provider, layout);
      
      // çµæœã®å¾Œå‡¦ç†
      const finalResult = await this.postProcessOCRResult(ocrResult, layout, {
        processingTime: Date.now() - startTime,
        imageHash,
        provider,
      });

      // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
      this.cacheResult(imageHash, finalResult);
      
      return finalResult;

    } catch (error) {
      console.error('OCR processing failed:', error);
      throw new Error(`OCRå‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: ${error.message}`);
    }
  }

  /**
   * SNSãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’ç”»åƒã‹ã‚‰åˆ¤å®š
   */
  private async detectSNSLayout(imageBuffer: Buffer): Promise<SNSLayout> {
    try {
      const metadata = await sharp(imageBuffer).metadata();
      const { width = 0, height = 0 } = metadata;
      
      // ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã«ã‚ˆã‚‹åˆ¤å®š
      const aspectRatio = width / height;
      
      // Instagram Stories (9:16)
      if (aspectRatio >= 0.5 && aspectRatio <= 0.6) {
        return {
          platform: 'instagram',
          confidence: 0.9,
          textRegions: [
            { name: 'caption', region: { x: 0.05, y: 0.7, width: 0.9, height: 0.25 }, priority: 1 },
            { name: 'username', region: { x: 0.05, y: 0.02, width: 0.6, height: 0.08 }, priority: 2 },
            { name: 'location', region: { x: 0.05, y: 0.1, width: 0.7, height: 0.05 }, priority: 3 },
          ],
          skipRegions: [
            { name: 'ui_elements', region: { x: 0.85, y: 0.02, width: 0.15, height: 0.15 } },
          ],
        };
      }
      
      // TikTok (9:16)
      if (aspectRatio >= 0.55 && aspectRatio <= 0.65) {
        return {
          platform: 'tiktok',
          confidence: 0.85,
          textRegions: [
            { name: 'description', region: { x: 0.05, y: 0.75, width: 0.7, height: 0.2 }, priority: 1 },
            { name: 'username', region: { x: 0.05, y: 0.85, width: 0.5, height: 0.06 }, priority: 2 },
            { name: 'hashtags', region: { x: 0.05, y: 0.91, width: 0.9, height: 0.08 }, priority: 3 },
          ],
          skipRegions: [
            { name: 'sidebar', region: { x: 0.85, y: 0.3, width: 0.15, height: 0.4 } },
            { name: 'bottom_ui', region: { x: 0, y: 0.95, width: 1, height: 0.05 } },
          ],
        };
      }
      
      // Twitter/X (1.91:1 or square)
      if (aspectRatio >= 0.9 && aspectRatio <= 2.1) {
        return {
          platform: 'twitter',
          confidence: 0.8,
          textRegions: [
            { name: 'tweet_text', region: { x: 0.05, y: 0.15, width: 0.9, height: 0.6 }, priority: 1 },
            { name: 'username', region: { x: 0.05, y: 0.05, width: 0.6, height: 0.08 }, priority: 2 },
            { name: 'timestamp', region: { x: 0.7, y: 0.05, width: 0.25, height: 0.05 }, priority: 3 },
          ],
          skipRegions: [
            { name: 'avatar', region: { x: 0.02, y: 0.02, width: 0.08, height: 0.12 } },
            { name: 'action_buttons', region: { x: 0, y: 0.85, width: 1, height: 0.15 } },
          ],
        };
      }

      // YouTube (16:9)
      if (aspectRatio >= 1.7 && aspectRatio <= 1.9) {
        return {
          platform: 'youtube',
          confidence: 0.75,
          textRegions: [
            { name: 'title', region: { x: 0.02, y: 0.75, width: 0.96, height: 0.1 }, priority: 1 },
            { name: 'description', region: { x: 0.02, y: 0.85, width: 0.7, height: 0.13 }, priority: 2 },
            { name: 'channel', region: { x: 0.02, y: 0.88, width: 0.4, height: 0.05 }, priority: 3 },
          ],
          skipRegions: [
            { name: 'video_player', region: { x: 0, y: 0, width: 1, height: 0.7 } },
          ],
        };
      }

      // ä¸€èˆ¬çš„ãªã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
      return {
        platform: 'general',
        confidence: 0.5,
        textRegions: [
          { name: 'main_content', region: { x: 0.05, y: 0.1, width: 0.9, height: 0.8 }, priority: 1 },
        ],
        skipRegions: [],
      };

    } catch (error) {
      console.error('SNS layout detection failed:', error);
      return {
        platform: 'general',
        confidence: 0.3,
        textRegions: [
          { name: 'full_image', region: { x: 0, y: 0, width: 1, height: 1 }, priority: 1 },
        ],
        skipRegions: [],
      };
    }
  }

  /**
   * ç”»åƒã‚’OCRç”¨ã«æœ€é©åŒ–
   */
  private async optimizeImageForOCR(
    imageBuffer: Buffer,
    layout: SNSLayout
  ): Promise<{ buffer: Buffer; optimizations: string[] }> {
    const optimizations: string[] = [];
    
    try {
      let image = sharp(imageBuffer);
      const metadata = await image.metadata();
      
      // ã‚µã‚¤ã‚ºæœ€é©åŒ–ï¼ˆè§£åƒåº¦ vs ã‚³ã‚¹ãƒˆ ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
      const maxDimension = 2048; // OCRã«ååˆ†ãªè§£åƒåº¦
      if (metadata.width! > maxDimension || metadata.height! > maxDimension) {
        image = image.resize(maxDimension, maxDimension, {
          fit: 'inside',
          withoutEnlargement: true,
        });
        optimizations.push('resize');
      }
      
      // ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãŒç›®çš„ãªã®ã§è‰²æƒ…å ±ã¯ä¸è¦ï¼‰
      image = image.grayscale();
      optimizations.push('grayscale');
      
      // ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ï¼ˆæ–‡å­—ã‚’èª­ã¿ã‚„ã™ãï¼‰
      image = image.normalize();
      optimizations.push('normalize');
      
      // ã‚·ãƒ£ãƒ¼ãƒ—åŒ–ï¼ˆæ–‡å­—ã®ã‚¨ãƒƒã‚¸ã‚’å¼·èª¿ï¼‰
      image = image.sharpen();
      optimizations.push('sharpen');
      
      // ãƒã‚¤ã‚ºé™¤å»ï¼ˆJPEGåœ§ç¸®ãƒã‚¤ã‚ºãªã©ï¼‰
      image = image.median(3);
      optimizations.push('denoise');
      
      // é–¢å¿ƒé ˜åŸŸã®ã¿ã‚’æŠ½å‡ºï¼ˆSNSãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
      if (layout.platform !== 'general' && layout.textRegions.length > 0) {
        const primaryRegion = layout.textRegions[0];
        const { region } = primaryRegion;
        
        // ç›¸å¯¾åº§æ¨™ã‚’çµ¶å¯¾åº§æ¨™ã«å¤‰æ›
        const left = Math.round(metadata.width! * region.x);
        const top = Math.round(metadata.height! * region.y);
        const width = Math.round(metadata.width! * region.width);
        const height = Math.round(metadata.height! * region.height);
        
        image = image.extract({ left, top, width, height });
        optimizations.push(`crop_${primaryRegion.name}`);
      }
      
      // æœ€çµ‚çš„ãªå“è³ªæœ€é©åŒ–
      const buffer = await image
        .png({ 
          quality: 90,
          compressionLevel: 6,
        })
        .toBuffer();
      
      return { buffer, optimizations };

    } catch (error) {
      console.error('Image optimization failed:', error);
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒã®ç”»åƒã‚’ãã®ã¾ã¾ä½¿ç”¨
      return { buffer: imageBuffer, optimizations: ['fallback'] };
    }
  }

  /**
   * æœ€é©ãªOCRãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠ
   */
  private selectOptimalProvider(options: {
    preferredProvider?: string;
    maxCost?: number;
  }): string {
    const { preferredProvider, maxCost = 10.0 } = options;
    
    // æ˜ç¤ºçš„ãªæŒ‡å®šãŒã‚ã‚‹å ´åˆ
    if (preferredProvider && this.ocrProviders[preferredProvider]?.enabled) {
      return preferredProvider;
    }
    
    // ã‚³ã‚¹ãƒˆåˆ¶é™ã«åŸºã¥ãé¸æŠ
    const availableProviders = Object.entries(this.ocrProviders)
      .filter(([_, config]) => config.enabled && config.costPerImage <= maxCost)
      .sort((a, b) => b[1].accuracy - a[1].accuracy); // ç²¾åº¦é †
    
    if (availableProviders.length === 0) {
      return 'tesseract'; // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    }
    
    return availableProviders[0][0];
  }

  /**
   * å®Ÿéš›ã®OCRå‡¦ç†ã‚’å®Ÿè¡Œ
   */
  private async performOCR(
    imageData: { buffer: Buffer; optimizations: string[] },
    provider: string,
    layout: SNSLayout
  ): Promise<OCRResult> {
    const startTime = Date.now();
    
    switch (provider) {
      case 'googleVision':
        return await this.performGoogleVisionOCR(imageData.buffer, layout);
      
      case 'azureCV':
        return await this.performAzureOCR(imageData.buffer, layout);
      
      case 'awsTextract':
        return await this.performAWSTextract(imageData.buffer, layout);
      
      case 'tesseract':
      default:
        return await this.performTesseractOCR(imageData.buffer, layout);
    }
  }

  /**
   * Google Vision API ã§OCR
   */
  private async performGoogleVisionOCR(imageBuffer: Buffer, layout: SNSLayout): Promise<OCRResult> {
    try {
      const response = await axios.post(
        `https://vision.googleapis.com/v1/images:annotate?key=${process.env.GOOGLE_VISION_API_KEY}`,
        {
          requests: [{
            image: {
              content: imageBuffer.toString('base64'),
            },
            features: [
              { type: 'TEXT_DETECTION', maxResults: 100 },
              { type: 'DOCUMENT_TEXT_DETECTION', maxResults: 1 },
            ],
            imageContext: {
              languageHints: ['ja', 'en'],
            },
          }],
        }
      );

      const annotations = response.data.responses[0];
      const fullText = annotations.fullTextAnnotation?.text || '';
      const textAnnotations = annotations.textAnnotations || [];

      const blocks: TextBlock[] = textAnnotations.slice(1).map((annotation: any) => ({
        text: annotation.description,
        confidence: 0.9, // Google Vision ã¯è©³ç´°ãªä¿¡é ¼åº¦ã‚’è¿”ã•ãªã„
        boundingBox: {
          x: annotation.boundingPoly.vertices[0].x || 0,
          y: annotation.boundingPoly.vertices[0].y || 0,
          width: (annotation.boundingPoly.vertices[2].x || 0) - (annotation.boundingPoly.vertices[0].x || 0),
          height: (annotation.boundingPoly.vertices[2].y || 0) - (annotation.boundingPoly.vertices[0].y || 0),
        },
        type: 'word' as const,
      }));

      return {
        text: fullText,
        confidence: 0.95,
        language: 'ja',
        blocks,
        metadata: {
          imageWidth: 0,
          imageHeight: 0,
          processingTime: 0,
          optimizationApplied: [],
          estimatedCost: this.ocrProviders.googleVision.costPerImage / 1000,
        },
      };

    } catch (error) {
      console.error('Google Vision OCR failed:', error);
      throw error;
    }
  }

  /**
   * Azure Computer Vision ã§OCR
   */
  private async performAzureOCR(imageBuffer: Buffer, layout: SNSLayout): Promise<OCRResult> {
    try {
      const endpoint = process.env.AZURE_CV_ENDPOINT;
      const key = process.env.AZURE_CV_KEY;

      const response = await axios.post(
        `${endpoint}/vision/v3.2/read/analyze`,
        imageBuffer,
        {
          headers: {
            'Ocp-Apim-Subscription-Key': key,
            'Content-Type': 'application/octet-stream',
          },
          params: {
            language: 'ja',
          },
        }
      );

      // Azure Read API ã¯éåŒæœŸãªã®ã§çµæœã‚’å–å¾—
      const operationLocation = response.headers['operation-location'];
      const operationId = operationLocation.split('/').pop();

      // çµæœã‚’å–å¾—ï¼ˆãƒãƒ¼ãƒªãƒ³ã‚°ï¼‰
      let result;
      for (let i = 0; i < 10; i++) {
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        const resultResponse = await axios.get(
          `${endpoint}/vision/v3.2/read/analyzeResults/${operationId}`,
          {
            headers: {
              'Ocp-Apim-Subscription-Key': key,
            },
          }
        );

        if (resultResponse.data.status === 'succeeded') {
          result = resultResponse.data;
          break;
        }
      }

      if (!result) {
        throw new Error('Azure OCR processing timeout');
      }

      const readResults = result.analyzeResult.readResults[0];
      const fullText = readResults.lines.map((line: any) => line.text).join('\n');
      
      const blocks: TextBlock[] = readResults.lines.map((line: any) => ({
        text: line.text,
        confidence: 0.93,
        boundingBox: {
          x: Math.min(...line.boundingBox.filter((_: any, i: number) => i % 2 === 0)),
          y: Math.min(...line.boundingBox.filter((_: any, i: number) => i % 2 === 1)),
          width: Math.max(...line.boundingBox.filter((_: any, i: number) => i % 2 === 0)) - 
                 Math.min(...line.boundingBox.filter((_: any, i: number) => i % 2 === 0)),
          height: Math.max(...line.boundingBox.filter((_: any, i: number) => i % 2 === 1)) - 
                  Math.min(...line.boundingBox.filter((_: any, i: number) => i % 2 === 1)),
        },
        type: 'line' as const,
      }));

      return {
        text: fullText,
        confidence: 0.93,
        language: 'ja',
        blocks,
        metadata: {
          imageWidth: readResults.width,
          imageHeight: readResults.height,
          processingTime: 0,
          optimizationApplied: [],
          estimatedCost: this.ocrProviders.azureCV.costPerImage / 1000,
        },
      };

    } catch (error) {
      console.error('Azure OCR failed:', error);
      throw error;
    }
  }

  /**
   * Tesseract ã§OCRï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
   */
  private async performTesseractOCR(imageBuffer: Buffer, layout: SNSLayout): Promise<OCRResult> {
    try {
      // Tesseract.js ã‚’ä½¿ç”¨ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
      const { createWorker } = require('tesseract.js');
      const worker = await createWorker();
      
      await worker.loadLanguage('jpn+eng');
      await worker.initialize('jpn+eng');
      
      const { data } = await worker.recognize(imageBuffer);
      await worker.terminate();

      const blocks: TextBlock[] = data.words.map((word: any) => ({
        text: word.text,
        confidence: word.confidence / 100,
        boundingBox: {
          x: word.bbox.x0,
          y: word.bbox.y0,
          width: word.bbox.x1 - word.bbox.x0,
          height: word.bbox.y1 - word.bbox.y0,
        },
        type: 'word' as const,
      }));

      return {
        text: data.text,
        confidence: data.confidence / 100,
        language: 'ja',
        blocks,
        metadata: {
          imageWidth: 0,
          imageHeight: 0,
          processingTime: 0,
          optimizationApplied: [],
          estimatedCost: 0,
        },
      };

    } catch (error) {
      console.error('Tesseract OCR failed:', error);
      // æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      return {
        text: '',
        confidence: 0,
        language: 'ja',
        blocks: [],
        metadata: {
          imageWidth: 0,
          imageHeight: 0,
          processingTime: 0,
          optimizationApplied: [],
          estimatedCost: 0,
        },
      };
    }
  }

  /**
   * AWS Textract ã§OCR
   */
  private async performAWSTextract(imageBuffer: Buffer, layout: SNSLayout): Promise<OCRResult> {
    try {
      // AWS SDKå®Ÿè£…ï¼ˆå®Ÿéš›ã®ç’°å¢ƒã§ã¯é©åˆ‡ã«è¨­å®šï¼‰
      // import { TextractClient, DetectDocumentTextCommand } from '@aws-sdk/client-textract';
      
      const response = await axios.post('https://textract.amazonaws.com/', {
        Document: {
          Bytes: imageBuffer.toString('base64'),
        },
        FeatureTypes: ['TABLES', 'FORMS'],
      }, {
        headers: {
          'Authorization': `AWS4-HMAC-SHA256 ${process.env.AWS_ACCESS_KEY_ID}`,
          'Content-Type': 'application/x-amz-json-1.1',
          'X-Amz-Target': 'Textract.DetectDocumentText',
        },
      });

      const blocks = response.data.Blocks || [];
      const textBlocks = blocks
        .filter((block: any) => block.BlockType === 'LINE')
        .map((block: any) => ({
          text: block.Text,
          confidence: block.Confidence / 100,
          boundingBox: {
            x: Math.round(block.Geometry.BoundingBox.Left * 1000),
            y: Math.round(block.Geometry.BoundingBox.Top * 1000),
            width: Math.round(block.Geometry.BoundingBox.Width * 1000),
            height: Math.round(block.Geometry.BoundingBox.Height * 1000),
          },
          type: 'line' as const,
        }));

      const fullText = textBlocks.map((block: any) => block.text).join('\n');

      return {
        text: fullText,
        confidence: 0.92,
        language: 'ja',
        blocks: textBlocks,
        metadata: {
          imageWidth: 0,
          imageHeight: 0,
          processingTime: 0,
          optimizationApplied: [],
          estimatedCost: this.ocrProviders.awsTextract.costPerImage / 1000,
        },
      };

    } catch (error) {
      console.error('AWS Textract OCR failed:', error);
      throw error;
    }
  }

  /**
   * OCRçµæœã®å¾Œå‡¦ç†
   */
  private async postProcessOCRResult(
    ocrResult: OCRResult,
    layout: SNSLayout,
    metadata: { processingTime: number; imageHash: string; provider: string }
  ): Promise<OCRResult> {
    
    // ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    const cleanedText = this.cleanupExtractedText(ocrResult.text, layout);
    
    // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    ocrResult.text = cleanedText;
    ocrResult.metadata.processingTime = metadata.processingTime;
    
    return ocrResult;
  }

  /**
   * æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
   */
  private cleanupExtractedText(text: string, layout: SNSLayout): string {
    let cleaned = text;
    
    // ä¸€èˆ¬çš„ãªãƒã‚¤ã‚ºé™¤å»
    cleaned = cleaned
      .replace(/[^\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\u3000-\u303Fa-zA-Z0-9\s\.,!?@#$%&()[\]{}":;]/g, '') // ä¸è¦ãªæ–‡å­—é™¤å»
      .replace(/\s+/g, ' ') // ç©ºç™½ã®æ­£è¦åŒ–
      .replace(/\n\s*\n/g, '\n') // ç©ºè¡Œã®é™¤å»
      .trim();
    
    // SNSç‰¹æœ‰ã®UIè¦ç´ ã‚’é™¤å»
    const uiPatterns = [
      /\d+\s*(ã„ã„ã­|likes?|â¤ï¸|â™¥ï¸)/gi,
      /\d+\s*(ã‚³ãƒ¡ãƒ³ãƒˆ|comments?|ğŸ’¬)/gi,
      /\d+\s*(ã‚·ã‚§ã‚¢|shares?|ğŸ”„)/gi,
      /\d+\s*(å†ç”Ÿ|views?|ğŸ‘€)/gi,
      /(ãƒ•ã‚©ãƒ­ãƒ¼|follow|following)/gi,
      /(ã‚¹ãƒˆãƒ¼ãƒªãƒ¼|story|stories)/gi,
      /ã‚‚ã£ã¨è¦‹ã‚‹/gi,
      /See more/gi,
    ];
    
    for (const pattern of uiPatterns) {
      cleaned = cleaned.replace(pattern, '');
    }
    
    return cleaned.trim();
  }

  /**
   * ç”»åƒã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ï¼‰
   */
  private generateImageHash(imageBuffer: Buffer): string {
    return createHash('md5').update(imageBuffer).digest('hex');
  }

  /**
   * ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—
   */
  private getCachedResult(imageHash: string): OCRResult | null {
    const cached = this.ocrCache.get(imageHash);
    if (cached && Date.now() - cached.timestamp < this.cacheExpiry) {
      return cached.result;
    }
    return null;
  }

  /**
   * çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
   */
  private cacheResult(imageHash: string, result: OCRResult): void {
    this.ocrCache.set(imageHash, {
      result,
      timestamp: Date.now(),
    });
  }

  /**
   * æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
   */
  private cleanupCache(): void {
    const now = Date.now();
    for (const [key, value] of this.ocrCache.entries()) {
      if (now - value.timestamp > this.cacheExpiry) {
        this.ocrCache.delete(key);
      }
    }
  }

  /**
   * ç”»åƒã®å¦¥å½“æ€§ã‚’æ¤œè¨¼
   */
  private validateImage(imageBuffer: Buffer): void {
    if (imageBuffer.length === 0) {
      throw new Error('ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™');
    }
    
    if (imageBuffer.length > this.maxFileSize) {
      throw new Error(`ç”»åƒã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ï¼ˆæœ€å¤§: ${this.maxFileSize / 1024 / 1024}MBï¼‰`);
    }
    
    // ç”»åƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ç¢ºèª
    const header = imageBuffer.subarray(0, 12);
    const isPNG = header.subarray(0, 8).equals(Buffer.from([0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A]));
    const isJPEG = header.subarray(0, 3).equals(Buffer.from([0xFF, 0xD8, 0xFF]));
    const isWebP = header.subarray(0, 4).equals(Buffer.from('RIFF', 'ascii')) && 
                   header.subarray(8, 12).equals(Buffer.from('WEBP', 'ascii'));
    
    if (!isPNG && !isJPEG && !isWebP) {
      throw new Error('ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ç”»åƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™ï¼ˆPNG, JPEG, WebP ã®ã¿å¯¾å¿œï¼‰');
    }
  }

  /**
   * ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šã‚’å–å¾—
   */
  getCostEstimate(provider: string): number {
    return this.ocrProviders[provider]?.costPerImage || 0;
  }

  /**
   * åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§ã‚’å–å¾—
   */
  getAvailableProviders(): Array<{ name: string; cost: number; accuracy: number }> {
    return Object.entries(this.ocrProviders)
      .filter(([_, config]) => config.enabled)
      .map(([name, config]) => ({
        name,
        cost: config.costPerImage,
        accuracy: config.accuracy,
      }));
  }
}

export const imageOCRService = new ImageOCRService();